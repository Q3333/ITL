# 하둡 5일차 (하이브)



### 실행시 알아둬야할 명령어



#### - 라이브 패널 2개가 아닐시

vi /etc/hosts 수정

vi /etc/sysconfig/iptables 수정

systemctl stop iptables

systemctl start iptables



#### -배쉬 프로파일 바로 적용 하는 법

source ./.bash_profile



#### -하둡 실행

cd /usr/local/hadoop~/sbin

./start-all.sh





# 하이브

하이브에서 운영체제 명령어 쓰는 법 : !

※ 세미콜론 필요! ;



ex ) !ls /home/hadoop/;



### 하이브의 특징

- HiveQL은  JOIN,  서브쿼리, UNION ALL을 지원한다.
- GROUP BY는  Reduce에서 처리하지만, Map에서 처리하도록 명시적으로 지정할 수 있다. 
-  Map 태스크의 메모리 사용량이 늘어날 가능성이 있지만, Map에서 집계할 수 있으므로 Reduce로 보내는 전송량을 줄 일 수 있다. 
- Map에서 GROUP BY를 실행하려면  hive.map.agg 속성을 true로 한다
- ORDER BY로 출력 결과 전체를 정렬할  때는 reducer 수가 하나로 제한한다
- SORT BY를 지정하면  Reducer를 다수 동작시킬 수 있으며, 처리 결과를 Reducer내에 정렬한다.





#### 오더바이와 소트바이의 차이



오더바이 : 

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190821/images/group.PNG>)

```
hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      ORDER BY Year,Month;   ----reducer개수 1개로 제한, 전체 정렬
```





소트 바이 : 

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190821/images/sort.PNG>)

```
hive> SELECT Year,Month, count(DepDelay)
      FROM airline
      GROUP BY Year,Month
      SORT BY Year,Month;   --reducer 별 처리 데이터 정렬, 전체 결과 정렬되지 않음
```







#### 데이터 집어넣는 구문

1. external 테이블 생성하고 location 지정

```
hive> CREATE EXTERNAL TABLE airline (
Year string,
Month string,
DayofMonth string,
DayOfWeek string,
DepTime string,
CRSDepTime string,
ArrTime string,
CRSArrTime string,
UniqueCarrier string,
FlightNum string,
TailNum string,
ActualElapsedTime string,
CRSElapsedTime string,
AirTime string,
ArrDelay string,
DepDelay string,
Origin string,
Dest string,
Distance string,
TaxiIn string,
TaxiOut string,
Cancelled string,
CancellationCode string,
Diverted string,
CarrierDelay string,
WeatherDelay string,
NASDelay string,
SecurityDelay string,
LateAircraftDelay  string
)
ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ',' 
 LINES TERMINATED BY '\n'
LOCATION '/data/airline/';
```





2. txt 를 read



```
[hadoop@master ~]$ vi /home/hadoop/dept.txt 

10,'ACCOUNTING','NEW YORK'
20,'RESEARCH','DALLAS'
30,'SALES','CHICAGO'
40,'OPERATIONS','BOSTON'


hive> CREATE TABLE IF NOT EXISTS dept (
deptno INT, dname STRING, loc STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

hive> describe dept

hive> load data local inpath '/home/hadoop/dept.txt' 
      overwrite into table dept;
hive> select  * from dept;
```





3. 조인해서 테이블 만들기



```
hive> CREATE TABLE airlineinfo 
 as
   select  a.UniqueCarrier  UniqueCarrier,
   b.CarrierFullName  CarrierFullName,
   a.FlightNum FlightNum,
   a.TailNum  TailNum ,
   a.Dest  Dest,
   a.Distance  Distance,
   a.Cancelled  Cancelled
 from  airline a , carriers b 
 where a.UniqueCarrier = substr(b.UniqueCarrier, 2, 2);

hive> select * from airlineinfo limit 10;
hive> !hadoop fs -ls /user/hive/warehouse/

hive> select count(*) from airlineinfo ;
hive> select count(*) from airline ;

```







## 하둡 R 연동



#### R명령어



setwd() = 현재 디렉토리 설정



NA = not available , sum 할때 NA가 있으면 무조건 NA가 뜸,

-> sum(1, 2, NA, na.rm=T)  # NA값을 제거하고 올바른 계산을 수행







### 1부터 1,000까지의 숫자를 생성/ 각 숫자 모두를 제곱하는 연산을 수행 



#### 하둡 스크립트로 내용 실행 : source("/home/hadoop/1to1000")

1to1000 내용 

```
library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce

dfs.rmr("/tmp/ex1")
small.ints <- to.dfs(1:1000, "/tmp/ex1")

result <- mapreduce(input = small.ints, 
	map = function(k,v) cbind(v,v^2)
)
out <- from.dfs(result)
print( out )

```



실행 화면

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190821/images/1to1000.PNG>)



##### ※ 배쉬 프로파일 변경했으면 반드시 source를 해줘야 변경 사항이 적용이 된다.





### 자료값이 0보다 큰 수와 작은 수의 빈도계산

##### 하둡 스크립트로 내용 실행 : source("/home/hadoop/mapR")

mapR 내용

```
library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
random <- to.dfs(rnorm(100))
map <- function(k,v) {
        key <- ifelse(v < 0, "less", "greater")
        keyval(key, 1)
}
reduce <- function(k,v) {
        keyval(k, length(v))
}
Freq <- mapreduce (
        input= random, output="/tmp/ex3",
        map = map, reduce = reduce
)
out <- from.dfs(Freq)
print(out)

```



실행 화면

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190821/images/mapr.PNG>)





### 문서자료에서 단어빈도계산

##### 하둡 스크립트로 내용 실행 : source("/home/hadoop/WordCount")

WordCount 내용

```
library(rhdfs) # Rhadoop package for hdfs
hdfs.init()    # Start to connect HDFS, 반드시 rmr2를 로드하기 전
library(rmr2)  # RHadoop package for MapReduce
 
inputfile <- "/tmp/README.txt"
if(!hdfs.exists(inputfile)) stop("File is not found")
outputfile <- "/tmp/ex4"
if(hdfs.exists(outputfile)) hdfs.rm(outputfile)
 
map <- function(key, val){
	words.vec <- unlist(strsplit(val, split = " "))
	#lapply(words.vec, function(word) 
    keyval(words.vec, 1)
}
 
reduce <- function(word, counts ) {
	keyval(word, sum(counts))
}

result <- mapreduce(input = inputfile,
	output = outputfile, 
	input.format = "text", 
	map = map, 
	reduce = reduce, 
	combine = T
)
 
## wordcount output
freq.dfs <- from.dfs(result)
freq <- freq.dfs$val
word <- freq.dfs$key
oidx <- order(freq, decreasing=T)[1:10]
 
# Words frequency plot
barplot(freq[oidx], names.arg=word[oidx] )

```





README 파일은 아무 기사에서 가져옴.





실행 화면

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190821/images/WC.PNG>)

