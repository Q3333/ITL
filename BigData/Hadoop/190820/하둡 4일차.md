# 하둡 4일차(조인,하이브)



배쉬 프로파일 바로 적용 하는 법

source ./.bash_profile



## 3일차 복습



### 분산 컴퓨팅

하나 이상의 호스트들의 접속을 받는 프로그램

(호스트는 자바일 경우 OS -> JDK ->App의 실행 순서를 가진다.)



#### 분산 컴퓨팅의 필요 조건



1. 장애 허용 -  분산 클러스터 노드 중 하나가 문제가 생겨도 메인 컴퓨팅 프로세스에  부정적인 영향 주지 않아야 한다. (프로세스 실패가 발생하지 않아야 한다)

  

2. 복구능력 - 분산 클러스터 노드에서 수행중인 작업이 실패하더라도 작업으로부터 어떤 데이터도 손실 되어서는 안된다.

  

3. 선형적 확장성 - 컴퓨팅 능력, 스토리지 공간 확장등, 성능도 선형적으로 증가해야 한다.



##### 하둡 아키텍처 : HDFS ,Yarn, MapReduce, API

##### 하둡 클러스터 :  하둡분산파일시스템(HDFS)과 클러스터 리소스 매니저(Yarn)를 기반으로 하는 하둡 소프트웨어를 사용하는 컴퓨터들의 집합



ex) 야후 의 경우 클러스터에 연결된 컴퓨터의 갯수가 10000개.



Hadoop2.0부터 마스터 노드 2개 이상 구성하여 고가용성 (HA) 을 지원한다.





##### 마스터 노드 (Active, Standby) :

- 하둡 클러스터의 작업을 중재
- 하둡 클라이언트는 파일을 저장, 읽고 , 처리하려면 master노드에 접속합니다.
- namenode가 구성되고, 파일을 저장, 쓰기 요청에 대해서  파일시스템의 메타 정보 관리
- mapreduce 작업의 중재하는 프로세스 JobTracker가 구성



#### 

##### 워커노드(슬레이브 노드) : 

- 마스터 노드의 지시를 받아서 명령을 수행 (실제 데이터를 저장하고, 데이터 처리 프로세싱 하는 노드, TaskTracker)





##### HDFS는 HDFS의 스토리지를 관리

NameNode : 

- HDFS 파일 시스템 디렉토리 트리와 파일의 위치 등 HDFS 스토리지 관련 메타 정보(블럭 데이터를 데이터 노드에 매핑)를 관리

- 파일 , 디렉토리, 생성, 열기, 쓰기 오퍼레이션 수행

- 어떤 데이터 노드에 복제되고, 복제 후에 삭제할지 결정

- 데이터 노드에서 보내온 하트비트와 블록 리포트를 처리 (블록 위치 유지, 데이터 노드의 상태 관리)



SecondaryNameNode : HDFS 스토리지 관련 메타 정보 업데이트 ( 기본 1시간 간격, fsimage파일과 editlog파일을 merge, 새로 fsimage 업데이트)



DataNode : 

- 마스터 노드에 접속 유지, 3초 간격으로 heartbit(살아있다는 신호), block report를 주기적으로 전송, 마스터 노드의 요청을 처리(block저장, block 삭제) 
- 로컬 파일 시스템에 블록을 저장
- 데이터에 대한 읽기, 쓰기 수행
- 데이터 블록 생성 및 삭제 수행
- 클러스터에 데이터 블럭 복제
- 주기적으로 하트비트와 블럭 리포트 전송







##### Yarn 서비스

resource manager : 

- 마스터 노드에서 실행,

- 클러스터의 리소스를 나눠주는 역할, 

- TaskTracker(워커노드에 존재)들의 Task를 스케줄링

  

node manager : 

워커 노드에서 실행, Task들을 실행시키고 관리, resource manager 와 관계 유지, 테스트 상태, 노드 상태 관리



application manager : 

클러스터에서의 메인이 되는 마스터 프로세스로서 어플리케이션 별로 하나씩 실행됨,

클러스터에서 실행되는 어플리케이션의 실행 조율, 리소스 매니저와 통신(관계 유지)하면서 리소스 조절





```
※ sharding : 같은 테이블 스키마를 가진 데이터를 다수의 데이터베이스에 분산하여 저장하는 방법을 의미
```

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190820/images/sha.PNG>)



하둡 클러스터에서 장애 허용과 복구 능력을 위해 sharding, replication을 수행한다.

배치 처리, 파일 기반 처리 (map의 처리 결과도 map처리된 datanode에 파일로 저장, reducer의 출력 결과도 hdfs에 저장, disk기반, stream기반, sequential하게 처리)



조인될 데이터의 키와 조인할 데이터의 키에 두 키를 구분할 수 있는 문자열을 추가하여 맵의 출력으로 보냄
리듀서에서 키에 따라 다른 키의 값으로 대체시킴





##### 매퍼에 들어가는 값?

Mapper<인풋키, 인풋밸류, 아웃풋키, 아웃풋밸류>









### 맵 사이드 조인 (리듀서 필요없음)



두개의 csv파일 간의 조인을 하는 방법.



1. 드라이버(메인)



```java
package lab.hadoop.join;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import lab.hadoop.delaycount.DelayCount;

public class MapsideJoin extends Configured implements Tool {
	
	public int run(String[] args) throws Exception{
		String[] otherArgs = new GenericOptionsParser(getConf(), args)
				.getRemainingArgs();
		
		//입력 출 데이터 경로 확인 
		if(otherArgs.length != 3) {
			System.err.println("Usage: MapsideJoin <metadata> <in> <out>");
			System.exit(2);
		}
		
		Configuration conf = new Configuration();
		
		//파일 시스템 제어 객체 생성
		FileSystem hdfs = FileSystem.get(conf);
		//경로 체크
		Path path = new Path(args[2]);
		if(hdfs.exists(path)) {
			hdfs.delete(path,true);
		}
		
		// Job 이름 설정 
		Job job = Job.getInstance(conf,"MapsideJoin");
		
		// 분산 캐시 설정
		DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(),
		job.getConfiguration());
		
		//입출력 데이터 경로 설정
		FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
		
		
		// Job 클래스 설정 
		job.setJarByClass(MapsideJoin.class);
		
		// Mapper 설정
		job.setMapperClass(MapperWithMapsideJoin.class);
		
		// Reducer 설정
		job.setNumReduceTasks(0);
		
		
		//입출력 데이터 포맷 설정
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		
		//출력키 및 출력값 유형 설정
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		job.waitForCompletion(true);
		return 0;
		
	}
	
	public static void main(String[] args) throws Exception {
		// Tool 인터페이스 실행
		int res = ToolRunner.run(new Configuration(), new MapsideJoin(), args);
		System.out.println("## RESULT:" + res);

	}
	
	
	
}

```





2. 매퍼

   

```java
package lab.hadoop.join;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.Hashtable;

import javax.swing.text.ChangedCharSetException;

import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class MapperWithMapsideJoin extends
	Mapper<LongWritable, Text, Text, Text>{	
	
	private Hashtable<String, String> joinMap = new Hashtable<String,String>();
	
	//map 출력키 
	private Text outputKey = new Text();



	@Override
	protected void setup(Context context)
			throws IOException, InterruptedException {
		try {
			// 분산캐시 조회
			Path[] cacheFiles = DistributedCache.getLocalCacheFiles(context
					.getConfiguration());
			// 조인 데이터 생성
			if (cacheFiles != null && cacheFiles.length > 0) {
				String line;
				String[] tokens;
				BufferedReader br = new BufferedReader(new FileReader(
						cacheFiles[0].toString()));
				try {
					while ((line = br.readLine()) != null) {
						tokens = line.toString().replaceAll("\"", "")
								.split(",");
						joinMap.put(tokens[0], tokens[1]);
					}
				} finally {
					br.close();
				}
			} else {
				System.out.println("### cache files is null!");
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public void map(LongWritable key, Text value, Context context)
			throws IOException, InterruptedException {

		if (key.get() > 0) {
			// 콤마 구분자 분리
			String[] colums = value.toString().split(",");
			if (colums != null && colums.length > 0) {
				try {
					outputKey.set(joinMap.get(colums[8]));
					context.write(outputKey, value);
				} catch (Exception e) {
					e.printStackTrace();
				}
			}
		}
	}
}


```







실행 결과



![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190820/images/mj2.PNG>)



![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190820/images/mj.PNG>)



00000의 실행 결과, 항공사 코드에서 항공사 이름으로 바뀌어서 출력되고있다.







### 리듀서 조인



1. 코드 매퍼

   ```java
   package lab.hadoop.join;
   
   import java.io.IOException;
   
   import org.apache.hadoop.io.LongWritable;
   import org.apache.hadoop.io.Text;
   import org.apache.hadoop.mapreduce.Mapper;
   
   public class CarrierCodeMapper extends Mapper<LongWritable, Text, Text, Text> {
   	//태그 선언
   	public final static String DATA_TAG = "A";
   	
   	private Text outputKey = new Text();
   	private Text outputValue = new Text();
   	
   	public void map(LongWritable key, Text value, Context context)
   		throws IOException, InterruptedException {
   	
   	if(key.get()>0) {
   		String[] colums = value.toString().replaceAll("\"","").split(",");
   		if (colums != null && colums.length > 0) {
   			outputKey.set(colums[0] + "_" + DATA_TAG);
   			outputValue.set(colums[1]);
   			context.write(outputKey, outputValue);
   		}
   	}
   }
   
   }
   
   ```

   



2. 매퍼

   ```java
   package lab.hadoop.join;
   
   import java.io.BufferedReader;
   
   public class MapperWithReducesideJoin extends
   	Mapper<LongWritable, Text, Text, Text>{	
   	
   	// 태그 선언 
   	public final static String DATA_TAG = "B";
   	
   	//map 출력키 
   	private Text outputKey = new Text();
   
   	public void map(LongWritable key, Text value, Context context)
   			throws IOException, InterruptedException {
   
   		if (key.get() > 0) {
   			// 콤마 구분자 분리
   			String[] colums = value.toString().split(",");
   			if (colums != null && colums.length > 0) {
   				try {
   					outputKey.set(colums[8] + "_" + DATA_TAG);
   					context.write(outputKey, value);
   				} catch (Exception e) {
   					e.printStackTrace();
   				}
   			}
   		}
   	}
   }
   
   
   ```

   



3. 리듀서

```java
package lab.hadoop.join;

import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class ReducerWithReducesideJoin extends Reducer<Text, Text, Text ,Text>{
	
	
	private Text outputKey = new Text();
	private Text outputValue = new Text();
	
	
	public void reduce(Text key, Iterable<Text> values, Context context)
		throws IOException, InterruptedException {
		//태그 조회
		String tagValue = key.toString().split("_")[1];
		
		for (Text value : values) {
			//출력 키 설정 
			if (tagValue.equals(CarrierCodeMapper.DATA_TAG)) {
				outputKey.set(value);
				// 출력값 설정 및 출력 데이터 생성
			} else if (tagValue.equals(MapperWithReducesideJoin.DATA_TAG)) {
				outputValue.set(value);
				context.write(outputKey, outputValue);
			}

		}

	}
}

```





4. 드라이버 , 메인 



```java
package lab.hadoop.join;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class ReducesideJoin extends Configured implements Tool {
	
	public int run(String[] args) throws Exception{
		String[] otherArgs = new GenericOptionsParser(getConf(), args)
				.getRemainingArgs();
		
		//입력 출 데이터 경로 확인 
		if(otherArgs.length != 3) {
			System.err.println("Usage: ReducesideJoin <metadata> <in> <out>");
			System.exit(2);
		}
		
		Configuration conf = new Configuration();
		
		//파일 시스템 제어 객체 생성
		FileSystem hdfs = FileSystem.get(conf);
		//경로 체크
		Path path = new Path(args[2]);
		if(hdfs.exists(path)) {
			hdfs.delete(path,true);
		}
		
		// Job 이름 설정 
		Job job = Job.getInstance(conf,"ReducesideJoin");
		
		
		//출력 데이터 경로 설정	
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));
		
		
		// Job 클래스 설정 
		job.setJarByClass(ReducesideJoin.class);

		// Reducer 설정
		job.setReducerClass(ReducerWithReducesideJoin.class);
		
		
		//입출력 데이터 포맷 설정CarrierCodeMapper
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		
		//출력키 및 출력값 유형 설정
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		
		//MultipleInputs 설정
		MultipleInputs.addInputPath(job, new Path(otherArgs[0]),
				TextInputFormat.class,CarrierCodeMapper.class);
		MultipleInputs.addInputPath(job, new Path(otherArgs[1]),
				TextInputFormat.class,MapperWithReducesideJoin.class);	
		
		job.waitForCompletion(true);
		return 0;
		
	}
	
	public static void main(String[] args) throws Exception {
		// Tool 인터페이스 실행
		int res = ToolRunner.run(new Configuration(), new ReducesideJoin(), args);
		System.out.println("## RESULT:" + res);

	}
	
}

```





실행 결과



![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190820/images/rj.PNG>)



![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190820/images/rj2.PNG>)



파일이 너무 크기 때문에 tail으로 10개씩 봄.





## 하이브 

- 데이터 웨어하우스지만 하둡 위에서 동작한다.



- MapReduce를 자바에서 프로그래밍 하는 것이 아닌 SQL을 사용함. 

  (MapReduce를 직관적 “SQL“기반의 하둡 에코 시스템중 하나인 “Hive“를 통해서 실행)

  - 다소 복잡한 “MR 프로그래밍”이 보다 친근하고, 직관적인 “SQL”의 지원
  - 다이나믹한 검색 조건 지정
  - 매번 “Name Node” 배포 없이 원격(Remote)에서 “MR Job”를 실행 지원



## 하이브의 모드



#### Hive 내장 모드

- 설정 변경을 하지 않는 기본 구성 DBMS로  Derby를 이용한다.
- 혼자서 테스트 용도로 사용하기에 적합한 구성



#### Hive 로컬 모드

- Hive 클라이언트와 메타스토어로부터 DBMS를 독립시키는 구성이다.
- DBMS는 JDBC를 통해 접속한다. 
- 로컬 모드에서는 다수의 접속을 동시에 허용하지만, Hive 클라이언트가 모드 같은 노드에 존재해야 한다. 



#### Hive 원격 모드

- DBMS뿐만 아니라 메타스토어도 독립시킨 구성이다.
- Hive 클라이언트가 Thrift API를 경유해서 원격으로 메타스토어에 접속할 수 있다





## Hive(Hadoop)과 RDBMS의 차이

- 온라인 처리에 부적합 (MapReduce의 잡 하나를 실행하면 아무것도 하지 않아도 오버헤드로 20~30초 정도 시간이 걸리므로…..일괄처리를 고속으로 실행하기 위한 것) 

  -> 하둡은 배치파일 처리형식임. 온라인처리형식 X 

- 인덱스 및 트랜잭션 기능이 없다 

- 롤백(rollback)처리가 없다. (복수의 HiveQL을 병렬 실행하여 그중 하나라도 실패하면, 사용자 스스로 잡 관리와 불필요한 처리 결과를 삭제해주어야 한다)

- MapReduce의 keep.failed.task.files 파라미터는 MapReduce 잡이 실패하면 MapReduce 프레임워크 중간 파일은 종료시에 삭제되도록 초기 설정이 되어 있다

- Hive에는 Update나 Delete문이 없다



## 하이브의 특징

- Hive 데이터는 HDFS 상의 파일로 존재하며, Hive 테이블은 HDFS 디렉토리로 존재한다
- Hive 데이터베이스나 스키마도 HDFS 상의 디렉터리로 존재한다. (/user/hive/warehouse/테이블명  디렉터로 존재)
- Hive 에서 컬럼이나 속성 등 테이블 실체가 아닌, 속성 정보에 해당하는 테이블 정의는 metastore라 불리며, RDBMS에 저장된다.
- Hive의 테이블 정의에서는 파티션이라 불리는 물리적 관리 단위를 지정할 수 있다
- 파티션은 HDFS 상의 디렉토리를 분할하는 것과 같다
- 파티션을 설정함으로써 처리 범위를 제어할 수 있어, 처리 고속화가 가능하다.
- 파티션 내의 모든 데이터가 필요 없어지면, 파티션 단위로 삭제할 수 있어서 관리도 수월하다





- HiveQL의 흐름은 Hive에서 쿼리문 앞에 EXPLAIN을 붙여 실행하면 확인할 수 있다

  (실행 계획 확인)

- HiveSQ은 Stage라는 단위로 MapReduce나 부속 처리로 변환되어, Stage 간 의존 관계가 생성된다





# 하이브 실행

##### \#HiveQL이 기술되어 있는 파일을 인수로 사용하여 명령어 라인에서 실행 

hive  -f  <파일명>     



##### \# 명령어 라인 인수로 HiveQL문을 직접 기술해서 실행

 hive  -e  ‘HiveSQL문’



##### \# Hive는 실행 시에 Hive에 관한 로그나 MapREduce 잡 실행 상태에 대한 정보를 출력하지만, -s 인수를 지정하면 silent 모드가 돼서 출력을 억제하는 것이 가능

 hive  -s  -e  ‘HiveSQL문’    



##### Hive 명령을 실행함으로써 Hive 쉘을 시작할 수 있다

##### SET 명령을 통해 Hadoop이나 Hive 관련 속성을 설정 할 수 있다.  (설정한 것은 Hive 쉘 내에서만 유효하다) 

```
hive> SET mapred.reduce.tasks= 20

Hive> SET  hive.parallel.queries=true;
```

