# 하둡 워드 카운트





1. 매퍼



```java
package hadoop.sample.wordcount;

import java.io.IOException;
import java.util.StringTokenizer;

 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Mapper;

 //Mapper클래스를 상속해야 함<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
 public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

 //1 값을 저장하기 위해 IntWritable 타입으로 선언(출력 값)
 private final static IntWritable one = new IntWritable(1);

 //단어를 저장하기 위해 Text 타입으로 선언(출력 타입)
 private Text word = new Text();

 //key : 입력 키, value : 입력 값
 //입력키는 라인번호, 입력 값은 라인의 문자열
 public void map(LongWritable key, Text value, Context context) throws IOException,
InterruptedException {
 //입력 문자열을 공백으로 분리하여 토큰화 함
 StringTokenizer itr = new StringTokenizer(value.toString());

 //토크나이저에 다음 토큰이 존재할 때까지 실행
 while (itr.hasMoreTokens()) {
 word.set(itr.nextToken()); //다음 토큰을 Text 타입 변수에 저장
 context.write(word, one); //Text타입에 저장된 값은 key, 1은 value로 저장
 }
 }
 }

```



2. 리듀서

```java
package hadoop.sample.wordcount;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

//리듀서는 Reducer를 상속받아 작성합니다.
 //Reducer<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
 public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

 //출력의 값으로 사용할 변수
 private IntWritable result = new IntWritable();

 //key : 입력 값, values : 동일키 일때의 값들
 //왜? values인가? 
 public void reduce(Text key, Iterable<IntWritable> values, Context context)
 throws IOException, InterruptedException {
 int sum = 0; //합을 저장하기 위한 변수
 for (IntWritable val : values) {//values안의 데이터를 소비하는 반복문
 //한번 실행할 때마다 values안에서 하나씩 빼와서 val에 저장

 //객체를 기본 타입으로 바꿔줘야 하기 때문에 val.get() 사용
 sum += val.get(); //IntWritable 클래스의 get 메서드는 안에 있는 값을 반환
 }
 result.set(sum); //결과는 기본타입 -> 객체에 저장
 context.write(key, result);//출력
 }
 }

```





3. 메인(드라이버)

   

```java
package hadoop.sample.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class WordCount {
 public static void main(String[] args) throws Exception {
 Configuration conf = new Configuration();
 Job job = new Job(conf, "WordCount");

 FileSystem fs = FileSystem.get(conf);//파일시스템 객체를 생성
 Path inputPath = new Path(args[0]);//입력 패스
 Path outputPath = new Path(args[1]); //출력 패스 
 if(fs.exists(outputPath)) {//만일 출력 패스가 존재하면
 fs.delete(outputPath, true);//출력 패스 삭제, 디렉토리이므로 true
 }

 job.setJarByClass(WordCount.class);//메인 클래스 지정
 job.setMapperClass(WordCountMapper.class);//메퍼 클래스 지정
 job.setReducerClass(WordCountReducer.class);//리듀서 클래스 지정

 job.setInputFormatClass(TextInputFormat.class);//입력 포맷지정
 job.setOutputFormatClass(TextOutputFormat.class);//출력 포맷 지정

 job.setOutputKeyClass(Text.class);//출력 키의 타입(단어는 텍스트)
 job.setOutputValueClass(IntWritable.class);//출력 값의 타입(갯수는 정수)

 FileInputFormat.addInputPath(job, inputPath);//입력 패스 지정
 FileOutputFormat.setOutputPath(job, outputPath);//출력 패스 지정

 job.waitForCompletion(true);//작업 실행 종료때까지 기다립니다.
 }
}

```





input.txt(기사 발췌)

```txt
마이리얼트립이 제주항공과 함께 특가 행사를 진행한다.

마이리얼트립과 제주항공은 오늘(19일)부터 오는 25일까지 제주항공 전 노선 한정 10만원대 또는 20만원대 특가를 실시한다고 밝혔다.

10만원대는 마카오, 코타키나발루, 세부, 사이판이며, 20만원대는 다낭, 하노이, 방콕, 대만이다. 다만 항공권 가격은 실시간으로 변동되거나 판매 마감될 수 있다.

탑승 기간은 19일부터 오는 12월 31일까지이며, 노선 별로 상이하다.

또 항공권 예약내역을 캡처한 후 인스타그램에 올린 전원에게는 스타벅스 기프티콘을 증정한다.
```





실행 결과 : 

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Hadoop/190819/images/WC.PNG>)



