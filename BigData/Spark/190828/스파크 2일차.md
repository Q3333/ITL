# Spark 2일차



## 1일차 복습

Spark?
인메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크

Spark 구성 요소?
- 클러스터 매니저 : Spark standalone, Yarn, Mesos
- SparkCore 
- Spark SQL
- Spark Streaming - 실시간 처리
- MLlib
- Graph X

Spark에서는 데이터 처리하기 추상화된 모델 : RDD(복구가능한 분산 데이터셋)

SparkApplication 구현 단계 :
1. SparkContext 생성
   - Spark애플리케이션과 Spark 클러스터와의 연결을 담당하는 객체
   - 모든 스파크 애플리케이션은 SparkContext를 이용해 RDD나 accumulator 또는 broadcast 변수 등을 다루게 됩니다.
   - Spark 애플리케이션을 수행하는 데 필요한 각종 설정 정보를 담는 역할을 한다
2. RDD (불변데이터 모델, parition가능) 생성
   collection, HDFS, hive , CSV 등..
3. RDD 처리 - 변환연산(RDD의 요소의 구조 변경, filter처리, grouping...)    
4. 집계, 요약 처리 - Action연산 
5. 영속화 

SparkApplication => Job
Spark 클러스터 환경에서 node들  : SparkClient, Master노드, Worker노드
SparkClient 역할 - SparkApplication 배포하고 실행을 요청
Spark Master노드 역할 -  Spark 클러스터 환경에서 사용가능한 리소스들의 관리
Spark Worker노드 역할 - 할당받은 리소스(CPU core, memory)를 사용해서  SparkApplication 실행 
Spark Worker노드에서 실행되는 프로세스 - Executor는 RDD의 partition을 task단위로 실행

Spark장점
1. 반복처리와 연속으로 이루어지는 변환처리를 고속화 (메모리 기반)
2. 딥러닝, 머신러닝등의 실행환경에 적합한 환경 제공
3. 서로 다른 실행환경과 구조, 데이터들의 처리에 대해서 통합 환경 제공

sc.textFile() : file로 부터 RDD생성
collect
map, flatMap()
mkString("구분자")







### 스파크 SQL



Dataframe을 다루는 가지 방법

1. DataFrame API를 이용하는 방법 (select,where 등)
2. DataFrame을 임시테이블에 등록하고 그 테이블에 쿼리를 발행하는 방법



- 스파크SQL은 정형화되지 않은 데이터 (ex 바이너리 ) 등을 처리하는 데는 적합하지 않다.



