# 스파크 1일차



### 스파크 실행 방법

```
spark-shell --master local verbose
```



### 스파크의 특징



1. 대량의 데이터를 고속 병렬분산처리한다.
2. 데이터 소스로부터 데이터를 읽어 들인 뒤, 스토리지 I/O와 네트워크 I/O를 최소화 하도록 처리한다.
3. 동일한 데이터에 대한 변환처리가 연속으로 이루어지는 경우와 머신러닝처럼 결과셋을 여러 번 반복 해 처리하는 경우에 적합.
4. 지연이 작게 동작하는 특성을 이용해 스트림 처리를 할 수도 있다.



특징은 크게 세가지 관점으로 분류한다.

- 반복처리와 연속으로 이루어지는 변환처리의 고속화
- 시행착오에 적합한 환경 제공
- 서로 다른 처리를 통합해 이용할 수 있는 환경



즉 단 한 번만 실행되는 변환ㅊ너리와 추출/가공처리에서는 스파크를 사용함에 있어 별 이득이 없다.





카프카(Kafka) : 데이터 수집 역할

- 사용자의 가공하지 않은 원래 그대로의 채널 선택 정보를 가져옴 







### 기본 데이터 모델 :



RDD

- 반환과 액션 이라는 두 종류의 처리를 적용할 수 있다. 
- 내부 요소 값이 변경 불가한 불변 성질이 있다.
- 생성이나 변환이 지연 평ㅇ가되는 성질이 있다.







### 스파크 설치



su - hadoop

cd/usr/local

tar zxvf /home/hadoop/Downloads/spark

ls -l (소유자 확인)

ln -s spark~~~ spark

ls - l (이름 짧게 한 심볼릭링크확인)

chown -R hadoop:hadoop spark (소유자 변경)



hadoop으로

vi .bash_profile



export SPARK_HOME=/usr/local/spark

export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop

export YARN_CONF_DIR=/usr/local/hadoop-2.7.7/etc/hadoop



PATH에 

bin:$SPARK_HOME/bin: 추가



로그아웃 후 로그인.



spark-shell

 --marster local verbose (마스터 포트가 안되면 방화벽에 열어줘야댐)







예제 1

```
//로컬 파일시스템에서 파일을 읽어들여서 RDD로 생성
scala> val file = sc.textFile("file:///usr/local/spark/README.md")
//RDD로부터 한 행(라인)단위로 처리 - 단어 분리 후 새로운 RDD 생성 저장
scala> val words = file.flatMap(_.split(" "))
//같은 단어끼리 모아서 요약(개수) 계산 - map형태로  단어와 출현횟수
scala> val result = words.countByValue     

scala> result.get("For").get

```

사진

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Spark/190827/images/0.PNG>)







예제 2 , range 만들기

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Spark/190827/images/1.PNG>)





예제 3, range 만들고 +1 하기

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Spark/190827/images/2.PNG>)





예제 4, range 합치기

![1564104618052](<https://github.com/Q3333/ITL/blob/master/BigData/Spark/190827/images/3.PNG>)





